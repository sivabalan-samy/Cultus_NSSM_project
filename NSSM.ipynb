{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1jyaUG4a6z_JwJhc8Jy4Y1PiRK_VDfj7R","authorship_tag":"ABX9TyO2dqnPnmzv2x/egdpkPeRL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wYerKwmQnw9Y"},"outputs":[],"source":["# run_all.py\n","# Advanced Time Series Forecasting with Neural State Space Models (NSSMs)\n","# This script generates data, trains a VAE-style NSSM and a baseline LSTM, and evaluates them.\n","# Requirements: Python 3.8+, torch, numpy, matplotlib, scikit-learn\n","# Install: pip install torch numpy matplotlib scikit-learn\n","\n","import os\n","import math\n","import json\n","from pathlib import Path\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.metrics import mean_squared_error, mean_absolute_error\n","import matplotlib.pyplot as plt\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","OUTDIR = Path(\"outputs\")\n","OUTDIR.mkdir(exist_ok=True)\n","\n","# -------------------- Dataset generation --------------------\n","def generate_multivariate_series(seq_len=1000, n_features=5, seed=0):\n","    np.random.seed(seed)\n","    t = np.arange(seq_len)\n","    data = np.zeros((seq_len, n_features), dtype=float)\n","    for f in range(n_features):\n","        trend = 0.001 * (f + 1) * t\n","        season = 0.5 * np.sin(2 * np.pi * (t / (50 + 10 * f)) + (f * 0.5))\n","        ar = np.zeros(seq_len)\n","        phi = 0.6 - 0.05 * f\n","        for i in range(1, seq_len):\n","            ar[i] = phi * ar[i - 1] + 0.05 * np.random.randn()\n","        noise = 0.2 * np.random.randn(seq_len)\n","        data[:, f] = trend + season + ar + noise\n","    return data\n","\n","# Sliding-window dataset\n","class TimeSeriesDataset(Dataset):\n","    def __init__(self, data, input_len=50, pred_len=10):\n","        self.data = data.astype(np.float32)\n","        self.input_len = input_len\n","        self.pred_len = pred_len\n","        self.seq_len, self.n_features = self.data.shape\n","        self.indices = []\n","        for i in range(0, self.seq_len - input_len - pred_len + 1):\n","            self.indices.append(i)\n","    def __len__(self):\n","        return len(self.indices)\n","    def __getitem__(self, idx):\n","        i = self.indices[idx]\n","        x = self.data[i:i + self.input_len]\n","        y = self.data[i + self.input_len:i + self.input_len + self.pred_len]\n","        return x, y\n","\n","# -------------------- NSSM (VAE-style amortized inference) --------------------\n","class Encoder(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, z_dim, n_layers=1):\n","        super().__init__()\n","        # bidirectional LSTM encoder\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, bidirectional=True)\n","        self.fc_mean = nn.Linear(hidden_dim * 2, z_dim)\n","        self.fc_logvar = nn.Linear(hidden_dim * 2, z_dim)\n","    def forward(self, x):\n","        # x: (B, T, D)\n","        out, _ = self.lstm(x)\n","        h = out[:, -1, :]\n","        mean = self.fc_mean(h)\n","        logvar = self.fc_logvar(h)\n","        return mean, logvar\n","\n","class TransitionNet(nn.Module):\n","    def __init__(self, z_dim, hidden_dim):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(z_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, z_dim)\n","        )\n","    def forward(self, z):\n","        return self.net(z)\n","\n","class ObservationNet(nn.Module):\n","    def __init__(self, z_dim, out_dim, hidden_dim):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(z_dim, hidden_dim),\n","            nn.ReLU(),\n","            nn.Linear(hidden_dim, out_dim)\n","        )\n","    def forward(self, z):\n","        return self.net(z)\n","\n","class NSSM(nn.Module):\n","    def __init__(self, input_dim, z_dim=8, enc_h=64, trans_h=64, obs_h=64):\n","        super().__init__()\n","        self.encoder = Encoder(input_dim, enc_h, z_dim)\n","        self.transition = TransitionNet(z_dim, trans_h)\n","        self.observation = ObservationNet(z_dim, input_dim, obs_h)\n","    def reparameterize(self, mu, logvar):\n","        std = torch.exp(0.5 * logvar)\n","        eps = torch.randn_like(std)\n","        return mu + eps * std\n","    def forward(self, x_context, pred_len=10):\n","        # x_context: (B, T, D)\n","        mu, logvar = self.encoder(x_context)\n","        z = self.reparameterize(mu, logvar)  # initial latent\n","        preds = []\n","        for t in range(pred_len):\n","            dz = self.transition(z)\n","            z = z + dz  # residual update\n","            y_hat = self.observation(z)\n","            preds.append(y_hat.unsqueeze(1))\n","        preds = torch.cat(preds, dim=1)\n","        return preds, mu, logvar\n","\n","# -------------------- Baseline LSTM predictor --------------------\n","class LSTMPredictor(nn.Module):\n","    def __init__(self, input_dim, hidden_dim=64, num_layers=1, pred_len=10):\n","        super().__init__()\n","        self.pred_len = pred_len\n","        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, input_dim * pred_len)\n","    def forward(self, x):\n","        out, _ = self.lstm(x)\n","        h = out[:, -1, :]\n","        out = self.fc(h)\n","        out = out.view(x.size(0), self.pred_len, -1)\n","        return out\n","\n","# -------------------- Training utilities --------------------\n","def loss_nssm(preds, target, mu, logvar, beta=1e-3):\n","    rec_loss = ((preds - target) ** 2).mean()\n","    kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n","    return rec_loss + beta * kl, rec_loss.item(), kl.item()\n","\n","def train_one_epoch_nssm(model, loader, opt):\n","    model.train()\n","    total_loss = 0.0\n","    total_rec = 0.0\n","    total_kl = 0.0\n","    for x, y in loader:\n","        x = x.to(device); y = y.to(device)\n","        opt.zero_grad()\n","        preds, mu, logvar = model(x, pred_len=y.size(1))\n","        loss, rec, kl = loss_nssm(preds, y, mu, logvar)\n","        loss.backward()\n","        opt.step()\n","        total_loss += loss.item() * x.size(0)\n","        total_rec += rec * x.size(0)\n","        total_kl += kl * x.size(0)\n","    n = len(loader.dataset)\n","    return total_loss / n, total_rec / n, total_kl / n\n","\n","def train_one_epoch_lstm(model, loader, opt, criterion):\n","    model.train()\n","    total_loss = 0.0\n","    for x, y in loader:\n","        x = x.to(device); y = y.to(device)\n","        opt.zero_grad()\n","        preds = model(x)\n","        loss = criterion(preds, y)\n","        loss.backward()\n","        opt.step()\n","        total_loss += loss.item() * x.size(0)\n","    return total_loss / len(loader.dataset)\n","\n","def evaluate_model(model, loader, model_type='nssm'):\n","    model.eval()\n","    Ys = []\n","    Yh = []\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x = x.to(device); y = y.to(device)\n","            if model_type == 'nssm':\n","                preds, _, _ = model(x, pred_len=y.size(1))\n","            else:\n","                preds = model(x)\n","            Ys.append(y.cpu().numpy())\n","            Yh.append(preds.cpu().numpy())\n","    Ys = np.concatenate(Ys, axis=0)\n","    Yh = np.concatenate(Yh, axis=0)\n","    rmse = math.sqrt(mean_squared_error(Ys.reshape(-1, Ys.shape[-1]), Yh.reshape(-1, Yh.shape[-1])))\n","    mae = mean_absolute_error(Ys.reshape(-1, Ys.shape[-1]), Yh.reshape(-1, Yh.shape[-1]))\n","    return rmse, mae, Ys, Yh\n","\n","# -------------------- Main run --------------------\n","def main():\n","    # Hyperparameters\n","    seq_len = 1000\n","    n_features = 5\n","    input_len = 50\n","    pred_len = 10\n","    batch_size = 32\n","    z_dim = 8\n","    epochs = 12  # increase for better results\n","\n","    data = generate_multivariate_series(seq_len=seq_len, n_features=n_features)\n","    np.savetxt(OUTDIR / 'sample_data.csv', data, delimiter=',')\n","\n","    # Train/test split by time\n","    train_ratio = 0.8\n","    split = int(seq_len * train_ratio)\n","    train_series = data[:split]\n","    test_series = data[split - input_len:]  # keep context for windows\n","\n","    train_ds = TimeSeriesDataset(train_series, input_len=input_len, pred_len=pred_len)\n","    test_ds = TimeSeriesDataset(test_series, input_len=input_len, pred_len=pred_len)\n","    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n","    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n","\n","    # NSSM\n","    nssm = NSSM(input_dim=n_features, z_dim=z_dim).to(device)\n","    opt_n = torch.optim.Adam(nssm.parameters(), lr=1e-3)\n","    for ep in range(epochs):\n","        loss, rec, kl = train_one_epoch_nssm(nssm, train_loader, opt_n)\n","        print(f\"[NSSM] Epoch {ep+1}/{epochs}: loss={loss:.6f} rec={rec:.6f} kl={kl:.6f}\")\n","    torch.save(nssm.state_dict(), OUTDIR / 'nssm.pth')\n","\n","    # Baseline LSTM\n","    lstm = LSTMPredictor(input_dim=n_features, hidden_dim=64, pred_len=pred_len).to(device)\n","    opt_l = torch.optim.Adam(lstm.parameters(), lr=1e-3)\n","    criterion = nn.MSELoss()\n","    for ep in range(epochs):\n","        loss_l = train_one_epoch_lstm(lstm, train_loader, opt_l, criterion)\n","        print(f\"[LSTM] Epoch {ep+1}/{epochs}: train_mse={loss_l:.6f}\")\n","    torch.save(lstm.state_dict(), OUTDIR / 'lstm.pth')\n","\n","    # Evaluate\n","    nssm_rmse, nssm_mae, Ys_n, Yh_n = evaluate_model(nssm, test_loader, model_type='nssm')\n","    lstm_rmse, lstm_mae, Ys_l, Yh_l = evaluate_model(lstm, test_loader, model_type='lstm')\n","    print(\"\\nEvaluation on test set:\")\n","    print(f\"NSSM RMSE={nssm_rmse:.6f}, MAE={nssm_mae:.6f}\")\n","    print(f\"LSTM RMSE={lstm_rmse:.6f}, MAE={lstm_mae:.6f}\")\n","\n","    results = {\n","        'nssm_rmse': float(nssm_rmse),\n","        'nssm_mae': float(nssm_mae),\n","        'lstm_rmse': float(lstm_rmse),\n","        'lstm_mae': float(lstm_mae)\n","    }\n","    (OUTDIR / 'results.json').write_text(json.dumps(results, indent=2))\n","\n","    # Quick plot comparing first test sample predictions (feature 0)\n","    plt.figure(figsize=(8, 4.5))\n","    gt = Ys_n[0, :, 0]\n","    p_n = Yh_n[0, :, 0]\n","    p_l = Yh_l[0, :, 0]\n","    T = np.arange(len(gt))\n","    plt.plot(T, gt, label='Ground truth')\n","    plt.plot(T, p_n, label='NSSM pred')\n","    plt.plot(T, p_l, label='LSTM pred')\n","    plt.legend()\n","    plt.title('First test sample - feature 0 predictions')\n","    plt.tight_layout()\n","    plt.savefig(OUTDIR / 'pred_compare.png', dpi=150)\n","    print(f\"Outputs saved to {OUTDIR.resolve()}\")\n","\n","if __name__ == '__main__':\n","    main()"]}]}